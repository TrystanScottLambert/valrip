"""
Python script to generate a yaml meta data file from a parquet file.
"""

import datetime
import os
from dataclasses import dataclass
from typing import List

import polars as pl
import json
import httpx

from pymaml.maml import MAMLBuilder

from .config import protected_words, filter_words, exceptions
from .metadata_validator import MinMax


@dataclass
class ColumnMetaData:
    name: str
    ucd: str | None
    data_type: str
    qc: MinMax | None
    unit: str = "--"
    info: str = "--"

    def _to_maml_dict(self) -> dict[str, str]:
        """
        Puts the column data into the format that can be passed to pymaml
        """
        qc = self.qc.__dict__ if self.qc else None
        maml_dict = self.__dict__
        maml_dict["qc"] = qc
        return maml_dict


@dataclass
class Skeleton:
    """
    Skeletal structure of what is expected in a maml file.
    """

    table: str
    fields: List["ColumnMetaData"]
    date: str = str(datetime.datetime.today()).split(" ")[0]
    license: str = "Copyright WAVES [Private]"

    def to_file(self, outfile: str) -> None:
        builder = MAMLBuilder("v1.1")
        builder.set("dataset", "--")
        builder.set("date", self.date)
        builder.set("MAML_version", "1.1")
        builder.set("table", self.table)
        builder.set("version", "--")
        builder.set("author", "--")
        for column in self.fields:
            builder.add("fields", column._to_maml_dict())

        maml = builder.build()
        maml.to_file(outfile, include_none=True)
        _clean_file(outfile)


def _clean_file(file_name: str) -> None:
    """
    Cleans the outputed maml file to produce a skeletal structure for WAVES specific MAML.

    Removes any place holder values for required fields, and cleans up non accepted fields.
    """
    with open(file_name) as file:
        lines = file.readlines()
    lines = [line.replace("--", "") for line in lines]
    forbidden_words = ["array_size:", "keyarray:", "extra:", "miss:"]
    for forbidden_word in forbidden_words:
        lines = [line for line in lines if forbidden_word not in line]
    good_lines = []
    for i, line in enumerate(lines):
        if line.strip()[:3] == "qc:":
            try:
                if lines[i + 1].strip()[:3] == "min":
                    good_lines.append(line)
            except IndexError:
                pass
        else:
            good_lines.append(line)

    with open(file_name, "w") as file:
        file.write(
            "# Autogenerated maml. Please see https://waves.wiki.org.au/en/Working-Groups/TWG7-WAVES-database/WAVES-metadata-format-MAML to understand the waves maml format. This comment can be deleted.\n"
        )
        file.writelines(good_lines)


def _scrape_ucd(column_name: str) -> str:
    """
    Helper function will try to guess the ucd from the protected_words and filter configs.
    """
    current_ucds = []
    for exception in exceptions:
        if exception.name in column_name:
            current_ucds += [exception.ucd]
    for protected_word in protected_words:
        if "_" in protected_word.name:
            if protected_word.name in column_name:
                current_ucds += protected_word.ucd
        else:
            for word in column_name.split("_"):
                if word == protected_word.name:
                    current_ucds += protected_word.ucd
    for filter_word in filter_words:
        if filter_word.name in column_name:
            current_ucds += [filter_word.secondary_ucd]
    full_ucds = list(dict.fromkeys(";".join(current_ucds).split(";")))
    return ";".join(full_ucds)


def _scrape_cds_ucd(column_name: str) -> str | None:
    """
    Makes a request to https://cdsweb.u-strasbg.fr/UCD/ucd-finder/ and returns best guess at ucd.
    """
    sanitized_string = column_name.translate(str.maketrans("-_.", "   "))
    re = httpx.get(
        f"https://cdsweb.u-strasbg.fr/UCD/ucd-finder/suggest?d={sanitized_string}"
    )
    re_dict = json.loads(re.text)
    try:
        return re_dict["ucd"][0]["ucd"]
    except IndexError:
        return None


def guess_ucd(column_name: str, web_search: bool = True) -> str | None:
    """
    Looks for a WAVES ucd if it exists or else scrapes the CDS website.
    """
    ucd = _scrape_ucd(column_name)
    if ucd == "" and web_search:
        ucd = _scrape_cds_ucd(column_name)
    return ucd


def fields_from_df(
    data_frame: pl.DataFrame, web_search: bool = True
) -> list[ColumnMetaData]:
    """
    Automatically generating as much field metadata as possible.

    This function will attempt to guess the ucd strings using the
    official WAVES lookup table. If the search cds is True
    then any other column names will make requests to the cds website.
    """
    column_names = data_frame.columns
    # We are lucky here that datacentral adopts the polars datatypes lower cased.
    data_types = [str(dtype).lower() for dtype in list(data_frame.dtypes)]

    mins = data_frame.min().row(0)
    maxs = data_frame.max().row(0)
    qcs = [
        MinMax(min, max) if not isinstance(min, str) else None
        for min, max in zip(mins, maxs)
    ]

    ucds = [guess_ucd(column_name, web_search) for column_name in column_names]
    units = []
    for column_name in column_names:
        possible_unit = "--"
        for protected_word in protected_words:
            if len(protected_word.name.split("_")) > 1:
                if protected_word.name in column_name:
                    if not possible_unit:
                        possible_unit = protected_word.unit[0]

            for word in column_name.split("_"):
                if word == protected_word.name:
                    if not possible_unit:
                        possible_unit = protected_word.unit[0]
        units.append(possible_unit)
    field_data = []

    for name, data_type, ucd, qc, unit in zip(
        column_names, data_types, ucds, qcs, units
    ):
        field_data.append(ColumnMetaData(name, ucd, data_type, qc, unit=unit))
    return field_data


def make_maml(file_name: str) -> None:
    """
    Creates the maml file from the .parquet file
    """
    base_name = os.path.basename(os.path.splitext(file_name)[0])
    df = pl.read_parquet(file_name)
    basic_maml = Skeleton(base_name, fields_from_df(df))
    basic_maml.to_file(file_name.replace(".parquet", ".maml"))
